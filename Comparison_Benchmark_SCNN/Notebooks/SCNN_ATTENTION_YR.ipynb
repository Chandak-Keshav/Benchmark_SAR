{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1711468190725,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "D5_qsP5fCBKm"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install scipy\n",
    "# !pip install random\n",
    "# !pip install pandas\n",
    "# !pip install math\n",
    "# !pip install os\n",
    "# !pip install scikit-learn\n",
    "# !pip install torch\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1711468191109,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "RkUv6C2AaqOT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 19:06:45.125723: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-11 19:06:46.639879: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch, math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import reduce\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from skimage import io, measure\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, concatenate, Reshape, LeakyReLU\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_ySW9czhD5Y"
   },
   "source": [
    "Setting the seed of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1711468191109,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "5cuA86GuhESf"
   },
   "outputs": [],
   "source": [
    "# def seed_torch(seed = 123):\n",
    "#     random.seed(seed)\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# seed_torch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jO_FCfwRm9am"
   },
   "source": [
    "# 1. Feature extraction network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1711468191109,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "SVRw577qaqzU"
   },
   "outputs": [],
   "source": [
    "# class FeatNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(FeatNet, self).__init__()\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(1, 16, 3, 1, 1)\n",
    "#         self.bn1_1 = nn.BatchNorm2d(16)\n",
    "\n",
    "#         self.conv1_1 = nn.Conv2d(16, 16, 3, 1, 1)\n",
    "#         self.bn1_1 = nn.BatchNorm2d(16)\n",
    "#         self.conv1_2 = nn.Conv2d(16, 16, 3, 1, 1)\n",
    "#         self.bn1_2 = nn.BatchNorm2d(16)\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 1 ,2, 1)\n",
    "\n",
    "#         self.conv2_1 = nn.Conv2d(32, 32, 3, 1, 1)\n",
    "#         self.bn2_1 = nn.BatchNorm2d(32)\n",
    "#         self.conv2_2 = nn.Conv2d(32, 32, 3, 1, 1)\n",
    "#         self.bn2_2 = nn.BatchNorm2d(32)\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 1, 2, 1)\n",
    "\n",
    "#         self.conv3_1 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "#         self.bn3_1 = nn.BatchNorm2d(64)\n",
    "#         self.conv3_2 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "#         self.bn3_2 = nn.BatchNorm2d(64)\n",
    "\n",
    "#         # Feature fusion\n",
    "#         self.conv_fusion1 = nn.Conv2d(16, 64, 1, 4, 2)\n",
    "#         self.conv_fusion2 = nn.Conv2d(32, 64, 1, 2, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x1 = self.conv1(x)\n",
    "#         x = F.relu(self.bn1_1(self.conv1_1(x1)))\n",
    "#         x_1 = F.relu(self.bn1_2(self.conv1_2(x)))\n",
    "#         x = x1 + x_1\n",
    "#         x2 = self.conv2(x)\n",
    "#         x = F.relu(self.bn2_1(self.conv2_1(x2)))\n",
    "#         x_2 = F.relu(self.bn2_2(self.conv2_2(x)))\n",
    "#         x = x2 + x_2\n",
    "#         x3 = self.conv3(x)\n",
    "#         x = F.relu(self.bn3_1(self.conv3_1(x3)))\n",
    "#         x_3 = F.relu(self.bn3_2(self.conv3_2(x)))\n",
    "#         return x_1, x_2, x_3\n",
    "\n",
    "# class FeatFuse(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(FeatFuse, self).__init__()\n",
    "\n",
    "#         self.conv_fusion1 = nn.Conv2d(16, 64, 1, 4, 3)\n",
    "#         self.conv_fusion2 = nn.Conv2d(32, 64, 1, 2, 1)\n",
    "\n",
    "#         self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.fc1 = nn.Linear(64, 8)\n",
    "#         self.fc2 = nn.Linear(8, 64*3)\n",
    "\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, x1, x2, x3):\n",
    "\n",
    "#         batch_size = x1.size(0)\n",
    "#         out_channels = x3.size(1)\n",
    "#         x1 = self.conv_fusion1(x1)\n",
    "#         x2 = self.conv_fusion2(x2)\n",
    "#         output = []\n",
    "#         output.append(x1)\n",
    "#         output.append(x2)\n",
    "#         output.append(x3)\n",
    "#         x = x1 + x2 + x3\n",
    "\n",
    "#         x = self.global_pool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "\n",
    "#         a_b = x.reshape(batch_size, 3, out_channels, -1)\n",
    "#         a_b = self.softmax(a_b)\n",
    "#         #the part of selection\n",
    "#         a_b = list(a_b.chunk(3, dim=1))#split to a and b\n",
    "#         a_b = list(map(lambda x:x.reshape(batch_size, out_channels, 1, 1), a_b))\n",
    "#         V = list(map(lambda x,y:x*y, output, a_b))\n",
    "#         V = reduce(lambda x,y:x+y, V)\n",
    "#         return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qIEQpWvnJj2"
   },
   "source": [
    "# 2. The proposed SAFNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1711468191110,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "-Hc5Au4Gbm0g"
   },
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "\n",
    "#         self.featnet = FeatNet()\n",
    "#         self.featfuse = FeatFuse()\n",
    "#         self.featnet1 = FeatNet()\n",
    "#         self.featfuse1 = FeatFuse()\n",
    "#         #self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.fc = nn.Linear(64, 2)\n",
    "\n",
    "#         self.global_pool1 = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.global_pool2 = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.fc1 = nn.Linear(64, 2)\n",
    "#         self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "\n",
    "#     def forward(self, x, y):\n",
    "\n",
    "#         x1_1, x1_2, x1_3 = self.featnet(x)\n",
    "#         x2_1, x2_2, x2_3 = self.featnet1(y)\n",
    "\n",
    "#         feat_11 = self.featfuse(x1_1, x1_2, x1_3)\n",
    "#         feat_22 = self.featfuse1(x2_1, x2_2, x2_3)\n",
    "#         feat_1 = self.global_pool1(feat_11)\n",
    "#         feat_2 = self.global_pool2(feat_22)\n",
    "#         feat_1 = feat_1.view(feat_1.size(0), -1)\n",
    "#         feat_2 = feat_2.view(feat_2.size(0), -1)\n",
    "#         feat_1 = self.fc1(feat_1)\n",
    "#         feat_2 = self.fc2(feat_2)\n",
    "\n",
    "#         feature_corr = self.xcorr_depthwise(feat_11, feat_22)\n",
    "#         feat = feature_corr.view(feature_corr.size(0), -1)\n",
    "#         #feat = global_pool(feature_corr)\n",
    "#         feat = self.fc(feat)\n",
    "#         return feat_1, feat_2, feat\n",
    "\n",
    "#     def xcorr_depthwise(self, x, kernel):\n",
    "\n",
    "#         batch = kernel.size(0)\n",
    "#         channel = kernel.size(1)\n",
    "#         x = x.view(1, batch*channel, x.size(2), x.size(3))\n",
    "#         kernel = kernel.view(batch*channel, 1, kernel.size(2), kernel.size(3))\n",
    "#         out = F.conv2d(x, kernel, groups=batch*channel)\n",
    "#         out = out.view(batch, channel, out.size(2), out.size(3))\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1711468191110,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "iLsdHg0ZqjhO"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljW0mDornMLP"
   },
   "source": [
    "# 3. Data processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1711468191110,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "NOnxQkLtbtq1"
   },
   "outputs": [],
   "source": [
    "def addZeroPadding(X, margin=2):\n",
    "    newX = np.zeros((\n",
    "        X.shape[0] + 2 * margin,\n",
    "        X.shape[1] + 2 * margin,\n",
    "        X.shape[2]\n",
    "              ))\n",
    "    newX[margin:X.shape[0]+margin, margin:X.shape[1]+margin, :] = X\n",
    "    return newX\n",
    "\n",
    "def createImgCube(X ,gt ,pos:list ,windowSize=25):\n",
    "    margin = (windowSize-1)//2\n",
    "    zeroPaddingX = addZeroPadding(X, margin=margin)\n",
    "    dataPatches = np.zeros((pos.__len__(), windowSize, windowSize, X.shape[2]))\n",
    "    if( pos[-1][1]+1 != X.shape[1] ):\n",
    "        nextPos = (pos[-1][0] ,pos[-1][1]+1)\n",
    "    elif( pos[-1][0]+1 != X.shape[0] ):\n",
    "        nextPos = (pos[-1][0]+1 ,0)\n",
    "    else:\n",
    "        nextPos = (0,0)\n",
    "    return np.array([zeroPaddingX[i:i+windowSize, j:j+windowSize, :] for i,j in pos ]),\\\n",
    "    np.array([gt[i,j] for i,j in pos]) ,\\\n",
    "    nextPos\n",
    "\n",
    "def createPos(shape:tuple, pos:tuple, num:int):\n",
    "    if (pos[0]+1)*(pos[1]+1)+num >shape[0]*shape[1]:\n",
    "        num = shape[0]*shape[1]-( (pos[0])*shape[1] + pos[1] )\n",
    "    return [(pos[0]+(pos[1]+i)//shape[1] , (pos[1]+i)%shape[1] ) for i in range(num) ]\n",
    "\n",
    "def createPosWithoutZero(hsi, gt):\n",
    "    print(\"uniques \",np.unique(gt))\n",
    "    mask = gt == 1\n",
    "    return [(i,j) for i , row  in enumerate(mask) for j , row_element in enumerate(row) if row_element]\n",
    "\n",
    "def splitTrainTestSet(X, gt, testRatio, randomState=111):\n",
    "\n",
    "    X_train, X_test, gt_train, gt_test = train_test_split(X, gt, test_size=testRatio, random_state=randomState, stratify=gt)\n",
    "    return X_train, X_test, gt_train, gt_test\n",
    "\n",
    "def createImgPatch(lidar, pos:list, windowSize=25):\n",
    "\n",
    "    margin = (windowSize-1)//2\n",
    "    zeroPaddingLidar = np.zeros((\n",
    "      lidar.shape[0] + 2 * margin,\n",
    "      lidar.shape[1] + 2 * margin\n",
    "            ))\n",
    "    zeroPaddingLidar[margin:lidar.shape[0]+margin, margin:lidar.shape[1]+margin] = lidar\n",
    "    return np.array([zeroPaddingLidar[i:i+windowSize, j:j+windowSize] for i,j in pos ])\n",
    "\n",
    "def minmax_normalize(array):\n",
    "    amin = np.min(array)\n",
    "    amax = np.max(array)\n",
    "    return (array - amin) / (amax - amin)\n",
    "def postprocess(res):\n",
    "    res_new = res\n",
    "    res = measure.label(res, connectivity=2)\n",
    "    num = res.max()\n",
    "    for i in range(1, num+1):\n",
    "        idy, idx = np.where(res==i)\n",
    "        if len(idy) <= 20:\n",
    "            res_new[idy, idx] = 0\n",
    "    return res_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Zwv_LyinO3t"
   },
   "source": [
    "# 4. Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3424,
     "status": "ok",
     "timestamp": 1711468257914,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "fjjz-kyvbxDT",
    "outputId": "381b3af1-c0b6-4152-b1b9-f2b9dccfbad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniques  [0 1 2]\n",
      "uniques  [0 1 2]\n",
      "uniques  [1 2]\n",
      "uniques  [1 2]\n",
      "torch.Size([19425, 1, 7, 7])\n",
      "torch.Size([4857, 1, 7, 7])\n",
      "torch.Size([83776, 1, 7, 7])\n",
      "Creating dataloader\n"
     ]
    }
   ],
   "source": [
    "windowSize = 7 # patch size\n",
    "class_num = 2\n",
    "testRatio = 0.2 # the ratio of Validation set\n",
    "trainRatio = 0.9 # the ratio of Training set selected from preclassification\n",
    "\n",
    "data_path = './input_data/'\n",
    "data_traingt = sio.loadmat(os.path.join('mask_train.mat'))['mask_train']\n",
    "data_testgt = sio.loadmat(os.path.join( 'mask_test.mat'))['mask_test']\n",
    "im1 = sio.loadmat(os.path.join('data_1.mat'))['data']\n",
    "im2 = sio.loadmat(os.path.join('data_2.mat'))['data']\n",
    "\n",
    "im1 = im1.reshape(im1.shape[0], im1.shape[1], 1)\n",
    "im2 = im2.reshape(im2.shape[0], im2.shape[1], 1)\n",
    "\n",
    "\n",
    "# data_traingt = np.array(im_gt)\n",
    "# data_testgt = np.array(im_gt)\n",
    "\n",
    "height , width, c = im1.shape\n",
    "\n",
    "# All pseudo-label set\n",
    "train_1, labels ,_ = createImgCube(im1, data_traingt, createPosWithoutZero(im1, data_traingt), windowSize=windowSize)\n",
    "train_2, _ ,_ = createImgCube(im2, data_traingt, createPosWithoutZero(im2, data_traingt), windowSize=windowSize)\n",
    "\n",
    "# training set selected from pseudo-label set\n",
    "train_1, _, train_labels, _ = splitTrainTestSet(train_1, labels, trainRatio, randomState=111)\n",
    "train_2, _, _, _ = splitTrainTestSet(train_2, labels, trainRatio, randomState=111)\n",
    "\n",
    "# data augmentation if need\n",
    "Xh = []\n",
    "Xl = []\n",
    "y = []\n",
    "for i in range(train_1.shape[0]):\n",
    "    Xh.append(train_1[i])\n",
    "    Xl.append(train_2[i])\n",
    "\n",
    "    noise = np.random.normal(0.0, 0.01, size=train_1[0].shape)\n",
    "    noise2 = np.random.normal(0.0, 0.01, size=train_2[0].shape)\n",
    "    Xh.append(np.flip(train_1[i] + noise, axis=1))\n",
    "    Xl.append(np.flip(train_2[i] + noise2, axis=1))\n",
    "\n",
    "    k = np.random.randint(4)\n",
    "    Xh.append(np.rot90(train_1[i], k=k))\n",
    "    Xl.append(np.rot90(train_2[i], k=k))\n",
    "\n",
    "    y.append(train_labels[i])\n",
    "    y.append(train_labels[i])\n",
    "    y.append(train_labels[i])\n",
    "\n",
    "labels = np.asarray(y, dtype=np.int8)\n",
    "train_1 = np.asarray(Xh, dtype=np.float32)\n",
    "train_2 = np.asarray(Xl,dtype=np.float32)\n",
    "train_1 = torch.from_numpy(train_1.transpose(0,3,1,2)).float()\n",
    "train_2 = torch.from_numpy(train_2.transpose(0,3,1,2)).float()\n",
    "\n",
    "# Select a partial validation set from the training set\n",
    "X_train, X_val, train_labels, val_labels = splitTrainTestSet(train_1, labels, testRatio, randomState=111)\n",
    "X_train_2, X_val_2, _, _ = splitTrainTestSet(train_2, labels, testRatio, randomState=111)\n",
    "\n",
    "# testing set\n",
    "X_test, test_labels ,_ = createImgCube(im1, data_traingt, createPosWithoutZero(im1, data_testgt), windowSize=windowSize)\n",
    "X_test_2, _ ,_ = createImgCube(im2, data_traingt, createPosWithoutZero(im2, data_testgt), windowSize=windowSize)\n",
    "X_test = torch.from_numpy(X_test.transpose(0,3,1,2)).float()\n",
    "X_test_2 = torch.from_numpy(X_test_2.transpose(0,3,1,2)).float()\n",
    "\n",
    "print (X_train.shape)\n",
    "print (X_val.shape)\n",
    "print (X_test.shape)\n",
    "print(\"Creating dataloader\")\n",
    "\n",
    "\"\"\" Training dataset\"\"\"\n",
    "class TrainDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = train_labels.shape[0]\n",
    "        self.hsi = torch.FloatTensor(X_train)\n",
    "        self.lidar = torch.FloatTensor(X_train_2)\n",
    "        self.labels = torch.LongTensor(train_labels - 1)\n",
    "    def __getitem__(self, index):\n",
    "        return self.hsi[index], self.lidar[index], self.labels[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\"\"\" Testing dataset\"\"\"\n",
    "class TestDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = test_labels.shape[0]\n",
    "        self.hsi = torch.FloatTensor(X_test)\n",
    "        self.lidar = torch.FloatTensor(X_test_2)\n",
    "        self.labels = torch.LongTensor(test_labels - 1)\n",
    "    def __getitem__(self, index):\n",
    "        return self.hsi[index], self.lidar[index], self.labels[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\"\"\" Validation dataset\"\"\"\n",
    "class ValDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = val_labels.shape[0]\n",
    "        self.hsi = torch.FloatTensor(X_val)\n",
    "        self.lidar = torch.FloatTensor(X_val_2)\n",
    "        self.labels = torch.LongTensor(val_labels - 1)\n",
    "    def __getitem__(self, index):\n",
    "        return self.hsi[index], self.lidar[index], self.labels[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# generate trainloader and valloader\n",
    "trainset = TrainDS()\n",
    "testset  = TestDS()\n",
    "valset = ValDS()\n",
    "train_loader = torch.utils.data.DataLoader(dataset = trainset, batch_size = 128, shuffle = True, num_workers = 0,drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = testset, batch_size = 128, shuffle = False, num_workers = 0,drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = valset, batch_size = 128, shuffle = False, num_workers = 0,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniques  [1 2]\n"
     ]
    }
   ],
   "source": [
    "changed_pixels_positions = createPosWithoutZero(im1, data_testgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83776"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(changed_pixels_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQAMHkFinTkl"
   },
   "source": [
    "# 6. Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1711468264630,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "mnxnPMpxb0XA"
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((label) * torch.pow(euclidean_distance, 2)+(1-label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n",
    "\n",
    "def calc_loss(x1, x2, outputs, labels, alpha):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss1 = criterion(outputs, labels)\n",
    "\n",
    "    contrastive = ContrastiveLoss()\n",
    "    loss2 = contrastive(x1, x2, labels)\n",
    "\n",
    "    loss_sum = loss1 + alpha* loss2\n",
    "    return loss_sum\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (inputs_1, inputs_2, labels) in enumerate(train_loader):\n",
    "\n",
    "        inputs_1, inputs_2 = inputs_1.to(device), inputs_2.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        feat_1, feat_2, outputs = model(inputs_1, inputs_2)\n",
    "        loss = calc_loss(feat_1, feat_2, outputs, labels, alpha = 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print('[Epoch: %d]   [loss avg: %.4f]   [current loss: %.4f]' %(epoch + 1, total_loss/(epoch+1), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    predicted_changes = []\n",
    "    test_labels_list = []\n",
    "    for inputs_1, inputs_2, labels in test_loader:\n",
    "        inputs_1, inputs_2 = inputs_1.to(device), inputs_2.to(device)\n",
    "        _, _, outputs = model(inputs_1, inputs_2)\n",
    "        outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "        predicted_changes.extend(outputs)\n",
    "        test_labels_list.extend(labels.numpy())\n",
    "\n",
    "    predicted_changes = np.array(predicted_changes)\n",
    "    test_labels = np.array(test_labels_list)\n",
    "    print(\"predicted changes\", len(predicted_changes), test_labels.shape, predicted_changes)\n",
    "\n",
    "    correct_predictions = (predicted_changes == test_labels)\n",
    "    accuracy = np.mean(correct_predictions) * 100\n",
    "    print('Accuracy: %.2f' % accuracy)\n",
    "\n",
    "    return accuracy, predicted_changes\n",
    "\n",
    "def test2(model, device, test_loader):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    for inputs_1, inputs_2, labels in test_loader:\n",
    "\n",
    "        inputs_1, inputs_2 = inputs_1.to(device), inputs_2.to(device)\n",
    "        _, _, outputs = model(inputs_1, inputs_2)\n",
    "        outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "\n",
    "        if count == 0:\n",
    "            y_pred_test =  outputs\n",
    "            test_labels = labels\n",
    "            count = 1\n",
    "        else:\n",
    "            y_pred_test = np.concatenate( (y_pred_test, outputs) )\n",
    "            test_labels = np.concatenate( (test_labels, labels) )\n",
    "    a = 0\n",
    "    for c in range(len(y_pred_test)):\n",
    "        if test_labels[c]==y_pred_test[c]:\n",
    "            a = a+1\n",
    "    acc = a/len(y_pred_test)*100\n",
    "    print('%.2f' %(a/len(y_pred_test)*100))\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCNN Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1]   [loss avg: 212616209.7433]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 2]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 3]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 4]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 5]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 6]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 7]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 8]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 9]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 10]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 11]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 12]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 13]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 14]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 15]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 16]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 17]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 18]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 19]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 20]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 21]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 22]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 23]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 24]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 25]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 26]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 27]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 28]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 29]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 30]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 31]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 32]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 33]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 34]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 35]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 36]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 37]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 38]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 39]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 40]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 41]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 42]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 43]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 44]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 45]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 46]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 47]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 48]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 49]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 50]   [loss avg: 0.0000]   [current loss: 0.0000]\n",
      "100.00\n",
      "Save model!\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = Net().to(device)\n",
    "\n",
    "# num_epochs = 2\n",
    "\n",
    "# params_to_update = list(model.parameters())\n",
    "\n",
    "# optimizer = torch.optim.Adam(params_to_update, lr=lr, betas=betas)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum,weight_decay=0.0005)\n",
    "\n",
    "# best_acc = 0\n",
    "# for epoch in range(num_epochs):\n",
    "#     train(model, device, train_loader, optimizer, epoch)\n",
    "#     # validation accuracy\n",
    "#     acc = test2(model, device, val_loader)\n",
    "#     if acc >= best_acc:\n",
    "#         best_acc = acc\n",
    "#         print(\"Save model!\")\n",
    "#         torch.save(model.state_dict(),'model/model.pth')\n",
    "\n",
    "\n",
    "class SCNNModel(nn.Module):\n",
    "    def __init__(self, image_height, image_width, image_channel):\n",
    "        super(SCNNModel, self).__init__()\n",
    "\n",
    "        self.activation_fn = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Convolutional block\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # Spatial Attention Mechanism\n",
    "        self.attention_weights = nn.Conv2d(512, 1, kernel_size=1)  # Adjust channels to match the concatenated feature maps\n",
    "        \n",
    "\n",
    "        # Separate paths for SAR image 1\n",
    "        self.sar_output1 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # Separate paths for SAR image 2\n",
    "        self.sar_output2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # Pairwise skipped connection\n",
    "        self.sar_output1_skip = nn.Conv2d(256, 256, kernel_size=1)\n",
    "        self.sar_output2_skip = nn.Conv2d(256, 256, kernel_size=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(768 * 2 * 2, 512),  # Adjusted according to the output of merged_output\n",
    "            self.activation_fn,\n",
    "            nn.Linear(512, 256),  # Changed from 256 to 128\n",
    "            self.activation_fn,\n",
    "            nn.Linear(256, image_height * image_width)  # Changed from 256 to 128\n",
    "        )\n",
    "\n",
    "        self.output_reshape = lambda x: x.view(-1, image_height, image_width)\n",
    "\n",
    "    def forward(self, sar_input1, sar_input2):\n",
    "        # Convolutional block\n",
    "        sar_output1 = self.conv_block(sar_input1)\n",
    "        sar_output2 = self.conv_block(sar_input2)\n",
    "\n",
    "        # Spatial Attention Mechanism\n",
    "        attention_weights = self.attention_weights(torch.cat((sar_output1, sar_output2), dim=1))\n",
    "        sar_output1_attended = sar_output1 * attention_weights\n",
    "        sar_output2_attended = sar_output2 * attention_weights\n",
    "\n",
    "        # Separate paths for SAR image 1\n",
    "        sar_output1 = self.sar_output1(sar_output1_attended)\n",
    "\n",
    "        # Separate paths for SAR image 2\n",
    "        sar_output2 = self.sar_output2(sar_output2_attended)\n",
    "\n",
    "        # Pairwise skipped connection\n",
    "        sar_output1_skip = self.sar_output1_skip(sar_output1)\n",
    "        merged_output = torch.cat((sar_output2, sar_output1_skip), dim=1)\n",
    "\n",
    "        # Pairwise skipped connection\n",
    "        sar_output2_skip = self.sar_output2_skip(sar_output2)\n",
    "        merged_output = torch.cat((merged_output, sar_output2_skip), dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        output = self.fc(merged_output)\n",
    "        output = self.output_reshape(output)\n",
    "\n",
    "        return output[0], output[1], output[2]\n",
    "\n",
    "\n",
    "\n",
    "image_height = image_width = 128\n",
    "image_channel = 1\n",
    "model = SCNNModel(image_height, image_width, image_channel)\n",
    "betas = (0.9, 0.999)\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available, else use CPU\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "params_to_update = list(model.parameters())\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum,weight_decay=0.0005)model.eval().to(device)\n",
    "\n",
    "# Define the number of epochs\n",
    " # Example: 10 epochs\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    # validation accuracy\n",
    "    acc = test2(model, device, val_loader)\n",
    "    if acc >= best_acc:\n",
    "        best_acc = acc\n",
    "        print(\"Save model!\")\n",
    "        torch.save(model.state_dict(),'model/model.pth')\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1236383,
     "status": "ok",
     "timestamp": 1711469505212,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "9AJM099jcAUj",
    "outputId": "fd010617-8c2e-4270-d0d5-238ff81e92af"
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = Net().to(device)\n",
    "\n",
    "# num_epochs = 50\n",
    "# lr = 0.001\n",
    "# momentum = 0.9\n",
    "# betas = (0.9, 0.999)\n",
    "\n",
    "# params_to_update = list(model.parameters())\n",
    "\n",
    "# # optimizer = torch.optim.Adam(params_to_update, lr=lr, betas=betas)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum,weight_decay=0.0005)\n",
    "\n",
    "# best_acc = 0\n",
    "# for epoch in range(num_epochs):\n",
    "#     train(model, device, train_loader, optimizer, epoch)\n",
    "#     acc = test(model, device, val_loader)\n",
    "#     if acc >= best_acc:\n",
    "#         best_acc = acc\n",
    "#         print(\"Save model!\")\n",
    "#         torch.save(model.state_dict(),'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbpq604ro7hW"
   },
   "source": [
    "# 7. Record the final change map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "mU6Aj1eilG0D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted changes 83712 (83712,) [0 0 0 ... 0 0 0]\n",
      "Accuracy: 95.75\n",
      "The final accuracy is  95.74612958715596\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = SCNNModel().eval().to(device)\n",
    "image_height = image_width = 128\n",
    "image_channel = 1\n",
    "\n",
    "model = SCNNModel(image_height, image_width, image_channel).eval().to(device)\n",
    "model.load_state_dict(torch.load('model/model.pth'))\n",
    "acc,predicted_changes = test(model, device, test_loader)\n",
    "# predicted_changes = predicted_changes[:len(changed_pixels_positions)]\n",
    "print ('The final accuracy is ', acc)\n",
    "# print(\"len of changed_pixels_positions and predicted_changes \", len(changed_pixels_positions), len(predicted_changes))\n",
    "\n",
    "binary_image = np.full_like(im1, 255, dtype=np.uint8)\n",
    "# print(\"change pixels: \", predicted_changes.shape)\n",
    "# print(predicted_changes.shape)\n",
    "# Map predicted changes to the corresponding indices in changed_pixels_positions\n",
    "for idx, value in enumerate(predicted_changes):\n",
    "    # print(idx, value)\n",
    "    if value == 0:\n",
    "        y, x = changed_pixels_positions[idx]\n",
    "        binary_image[y, x] = 0  # Set as black for unchanged pixels\n",
    "    elif value == 1:\n",
    "        y, x = changed_pixels_positions[idx]\n",
    "        binary_image[y, x] = 255  # Set as white for changed pixels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualize the binary image\n",
    "plt.imshow(binary_image[:, :, 0], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Predicted Change Map')\n",
    "plt.savefig(\"SCNN_ATTENTION_YR_95.75%.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
