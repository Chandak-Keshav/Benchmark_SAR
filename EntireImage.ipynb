{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1711468190725,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "D5_qsP5fCBKm"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install scipy\n",
    "# !pip install random\n",
    "# !pip install pandas\n",
    "# !pip install math\n",
    "# !pip install os\n",
    "# !pip install scikit-learn\n",
    "# !pip install torch\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1711468191109,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "RkUv6C2AaqOT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch, math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import reduce\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from skimage import io, measure\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, concatenate, Reshape, LeakyReLU\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_ySW9czhD5Y"
   },
   "source": [
    "Setting the seed of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1711468191109,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "5cuA86GuhESf"
   },
   "outputs": [],
   "source": [
    "# def seed_torch(seed = 123):\n",
    "#     random.seed(seed)\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# seed_torch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qIEQpWvnJj2"
   },
   "source": [
    "# 2. SAFNet Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1711468191109,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "SVRw577qaqzU"
   },
   "outputs": [],
   "source": [
    "class FeatNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1, 1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv1_1 = nn.Conv2d(16, 16, 3, 1, 1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(16)\n",
    "        self.conv1_2 = nn.Conv2d(16, 16, 3, 1, 1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, 1 ,2, 1)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(32, 32, 3, 1, 1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(32)\n",
    "        self.conv2_2 = nn.Conv2d(32, 32, 3, 1, 1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, 1, 2, 1)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(64)\n",
    "        self.conv3_2 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Feature fusion\n",
    "        self.conv_fusion1 = nn.Conv2d(16, 64, 1, 4, 2)\n",
    "        self.conv_fusion2 = nn.Conv2d(32, 64, 1, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x = F.relu(self.bn1_1(self.conv1_1(x1)))\n",
    "        x_1 = F.relu(self.bn1_2(self.conv1_2(x)))\n",
    "        x = x1 + x_1\n",
    "        x2 = self.conv2(x)\n",
    "        x = F.relu(self.bn2_1(self.conv2_1(x2)))\n",
    "        x_2 = F.relu(self.bn2_2(self.conv2_2(x)))\n",
    "        x = x2 + x_2\n",
    "        x3 = self.conv3(x)\n",
    "        x = F.relu(self.bn3_1(self.conv3_1(x3)))\n",
    "        x_3 = F.relu(self.bn3_2(self.conv3_2(x)))\n",
    "        return x_1, x_2, x_3\n",
    "\n",
    "class FeatFuse(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatFuse, self).__init__()\n",
    "\n",
    "        self.conv_fusion1 = nn.Conv2d(16, 64, 1, 4, 3)\n",
    "        self.conv_fusion2 = nn.Conv2d(32, 64, 1, 2, 1)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(64, 8)\n",
    "        self.fc2 = nn.Linear(8, 64*3)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "\n",
    "        batch_size = x1.size(0)\n",
    "        out_channels = x3.size(1)\n",
    "        x1 = self.conv_fusion1(x1)\n",
    "        x2 = self.conv_fusion2(x2)\n",
    "        output = []\n",
    "        output.append(x1)\n",
    "        output.append(x2)\n",
    "        output.append(x3)\n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        a_b = x.reshape(batch_size, 3, out_channels, -1)\n",
    "        a_b = self.softmax(a_b)\n",
    "        #the part of selection\n",
    "        a_b = list(a_b.chunk(3, dim=1))#split to a and b\n",
    "        a_b = list(map(lambda x:x.reshape(batch_size, out_channels, 1, 1), a_b))\n",
    "        V = list(map(lambda x,y:x*y, output, a_b))\n",
    "        V = reduce(lambda x,y:x+y, V)\n",
    "        return V\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.featnet = FeatNet()\n",
    "        self.featfuse = FeatFuse()\n",
    "        self.featnet1 = FeatNet()\n",
    "        self.featfuse1 = FeatFuse()\n",
    "        #self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(64, 2)\n",
    "\n",
    "        self.global_pool1 = nn.AdaptiveAvgPool2d(1)\n",
    "        self.global_pool2 = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(64, 2)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        x1_1, x1_2, x1_3 = self.featnet(x)\n",
    "        x2_1, x2_2, x2_3 = self.featnet1(y)\n",
    "\n",
    "        feat_11 = self.featfuse(x1_1, x1_2, x1_3)\n",
    "        feat_22 = self.featfuse1(x2_1, x2_2, x2_3)\n",
    "        feat_1 = self.global_pool1(feat_11)\n",
    "        feat_2 = self.global_pool2(feat_22)\n",
    "        feat_1 = feat_1.view(feat_1.size(0), -1)\n",
    "        feat_2 = feat_2.view(feat_2.size(0), -1)\n",
    "        feat_1 = self.fc1(feat_1)\n",
    "        feat_2 = self.fc2(feat_2)\n",
    "\n",
    "        feature_corr = self.xcorr_depthwise(feat_11, feat_22)\n",
    "        feat = feature_corr.view(feature_corr.size(0), -1)\n",
    "        #feat = global_pool(feature_corr)\n",
    "        feat = self.fc(feat)\n",
    "        # print(feat_1.shape, feat_2.shape, feat.shape)\n",
    "        return feat_1, feat_2, feat\n",
    "\n",
    "    def xcorr_depthwise(self, x, kernel):\n",
    "\n",
    "        batch = kernel.size(0)\n",
    "        channel = kernel.size(1)\n",
    "        x = x.view(1, batch*channel, x.size(2), x.size(3))\n",
    "        kernel = kernel.view(batch*channel, 1, kernel.size(2), kernel.size(3))\n",
    "        out = F.conv2d(x, kernel, groups=batch*channel)\n",
    "        out = out.view(batch, channel, out.size(2), out.size(3))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1711468191110,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "iLsdHg0ZqjhO"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCNNModel(nn.Module):\n",
    "    def __init__(self, image_height, image_width, image_channel):\n",
    "        super(SCNNModel, self).__init__()\n",
    "\n",
    "        self.activation_fn = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Convolutional block\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # Separate paths for SAR image 1\n",
    "        self.sar_output1 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),  \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),   \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),    \n",
    "            self.activation_fn,\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Separate paths for SAR image 2\n",
    "        self.sar_output2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),  \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),   \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),    \n",
    "            self.activation_fn,\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Pairwise skipped connection\n",
    "        self.sar_output1_skip = nn.Linear(128, 256)\n",
    "        self.sar_output2_skip = nn.Linear(128, 256)\n",
    "\n",
    "        # Fully connected layers for prediction\n",
    "        self.fc_prediction = nn.Sequential(\n",
    "            nn.Linear(32 * 4 * 2, 512),  # Adjusted input size\n",
    "            self.activation_fn,\n",
    "            nn.Linear(512, 256),           # Adjusted output channels\n",
    "            self.activation_fn,\n",
    "            nn.Linear(256, 2)              # Output size is (batch_size, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, sar_input1, sar_input2):\n",
    "        # Convolutional block\n",
    "        sar_output1 = self.conv_block(sar_input1)\n",
    "        sar_output2 = self.conv_block(sar_input2)\n",
    "\n",
    "        # Separate paths for SAR image 1\n",
    "        sar_output1 = self.sar_output1(sar_output1)\n",
    "\n",
    "        # Separate paths for SAR image 2\n",
    "        sar_output2 = self.sar_output2(sar_output2)\n",
    "\n",
    "        # Pairwise skipped connection\n",
    "        sar_output1_skip = self.sar_output1_skip(sar_output1)\n",
    "        merged_output = torch.cat((sar_output2, sar_output1_skip), dim=1)\n",
    "\n",
    "        # Pairwise skipped connection\n",
    "        sar_output2_skip = self.sar_output2_skip(sar_output2)\n",
    "        merged_output = torch.cat((merged_output, sar_output2_skip), dim=1)\n",
    "\n",
    "        # Fully connected layers for prediction\n",
    "        merged_output = torch.cat((sar_output1, sar_output2), dim=1)\n",
    "        predictions = self.fc_prediction(merged_output)\n",
    "\n",
    "        return sar_output1, sar_output2, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCNN with Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCNNAttentionModel(nn.Module):\n",
    "    def __init__(self, image_height, image_width, image_channel):\n",
    "        super(SCNNAttentionModel, self).__init__()\n",
    "\n",
    "        # Spatial Attention Mechanism\n",
    "        self.attention_weights = nn.Conv2d(512, 1, kernel_size=1)  # Adjust channels to match the concatenated feature maps\n",
    "        \n",
    "\n",
    "        self.activation_fn = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Convolutional block\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # Separate paths for SAR image 1\n",
    "        self.sar_output1 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),  \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),   \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),    \n",
    "            self.activation_fn,\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Separate paths for SAR image 2\n",
    "        self.sar_output2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),  \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),   \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),    \n",
    "            self.activation_fn,\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Pairwise skipped connection\n",
    "        self.sar_output1_skip = nn.Linear(128, 256)\n",
    "        self.sar_output2_skip = nn.Linear(128, 256)\n",
    "\n",
    "        # Fully connected layers for prediction\n",
    "        self.fc_prediction = nn.Sequential(\n",
    "            nn.Linear(32 * 4 * 2, 512),  # Adjusted input size\n",
    "            self.activation_fn,\n",
    "            nn.Linear(512, 256),           # Adjusted output channels\n",
    "            self.activation_fn,\n",
    "            nn.Linear(256, 2)              # Output size is (batch_size, 2)\n",
    "        )\n",
    "\n",
    "        self.output_reshape = lambda x: x.view(-1, image_height, image_width)\n",
    "\n",
    "    def forward(self, sar_input1, sar_input2):\n",
    "        # Convolutional block\n",
    "        sar_output1 = self.conv_block(sar_input1)\n",
    "        sar_output2 = self.conv_block(sar_input2)\n",
    "\n",
    "        # Spatial Attention Mechanism\n",
    "        attention_weights = self.attention_weights(torch.cat((sar_output1, sar_output2), dim=1))\n",
    "        sar_output1_attended = sar_output1 * attention_weights\n",
    "        sar_output2_attended = sar_output2 * attention_weights\n",
    "\n",
    "        # Separate paths for SAR image 1\n",
    "        sar_output1 = self.sar_output1(sar_output1_attended)\n",
    "\n",
    "        # Separate paths for SAR image 2\n",
    "        sar_output2 = self.sar_output2(sar_output2_attended)\n",
    "        \n",
    "        # Pairwise skipped connection\n",
    "        sar_output1_skip = self.sar_output1_skip(sar_output1)\n",
    "        merged_output = torch.cat((sar_output2, sar_output1_skip), dim=1)\n",
    "\n",
    "        # Pairwise skipped connection\n",
    "        sar_output2_skip = self.sar_output2_skip(sar_output2)\n",
    "        merged_output = torch.cat((merged_output, sar_output2_skip), dim=1)\n",
    "\n",
    "        # Fully connected layers for prediction\n",
    "        merged_output = torch.cat((sar_output1, sar_output2), dim=1)\n",
    "        predictions = self.fc_prediction(merged_output)\n",
    "\n",
    "        return sar_output1, sar_output2, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljW0mDornMLP"
   },
   "source": [
    "# 3. Data processing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keep gt > for Yellow River and gt >= 0 for Ottawa Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPosWithoutZero(hsi, gt):\n",
    "    print(\"uniques \",np.unique(gt))\n",
    "    mask = gt > 0\n",
    "    return [(i,j) for i , row  in enumerate(mask) for j , row_element in enumerate(row) if row_element]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1711468191110,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "NOnxQkLtbtq1"
   },
   "outputs": [],
   "source": [
    "def addZeroPadding(X, margin=2):\n",
    "    newX = np.zeros((\n",
    "        X.shape[0] + 2 * margin,\n",
    "        X.shape[1] + 2 * margin,\n",
    "        X.shape[2]\n",
    "              ))\n",
    "    newX[margin:X.shape[0]+margin, margin:X.shape[1]+margin, :] = X\n",
    "    return newX\n",
    "\n",
    "def createImgCube(X ,gt ,pos:list ,windowSize=25):\n",
    "    margin = (windowSize-1)//2\n",
    "    zeroPaddingX = addZeroPadding(X, margin=margin)\n",
    "    dataPatches = np.zeros((pos.__len__(), windowSize, windowSize, X.shape[2]))\n",
    "    if( pos[-1][1]+1 != X.shape[1] ):\n",
    "        nextPos = (pos[-1][0] ,pos[-1][1]+1)\n",
    "    elif( pos[-1][0]+1 != X.shape[0] ):\n",
    "        nextPos = (pos[-1][0]+1 ,0)\n",
    "    else:\n",
    "        nextPos = (0,0)\n",
    "    return np.array([zeroPaddingX[i:i+windowSize, j:j+windowSize, :] for i,j in pos ]),\\\n",
    "    np.array([gt[i,j] for i,j in pos]) ,\\\n",
    "    nextPos\n",
    "\n",
    "def createPos(shape:tuple, pos:tuple, num:int):\n",
    "    if (pos[0]+1)*(pos[1]+1)+num >shape[0]*shape[1]:\n",
    "        num = shape[0]*shape[1]-( (pos[0])*shape[1] + pos[1] )\n",
    "    return [(pos[0]+(pos[1]+i)//shape[1] , (pos[1]+i)%shape[1] ) for i in range(num) ]\n",
    "\n",
    "\n",
    "\n",
    "def splitTrainTestSet(X, gt, testRatio, randomState=111):\n",
    "\n",
    "    X_train, X_test, gt_train, gt_test = train_test_split(X, gt, test_size=testRatio, random_state=randomState, stratify=gt)\n",
    "    return X_train, X_test, gt_train, gt_test\n",
    "\n",
    "def createImgPatch(lidar, pos:list, windowSize=25):\n",
    "\n",
    "    margin = (windowSize-1)//2\n",
    "    zeroPaddingLidar = np.zeros((\n",
    "      lidar.shape[0] + 2 * margin,\n",
    "      lidar.shape[1] + 2 * margin\n",
    "            ))\n",
    "    zeroPaddingLidar[margin:lidar.shape[0]+margin, margin:lidar.shape[1]+margin] = lidar\n",
    "    return np.array([zeroPaddingLidar[i:i+windowSize, j:j+windowSize] for i,j in pos ])\n",
    "\n",
    "def minmax_normalize(array):\n",
    "    amin = np.min(array)\n",
    "    amax = np.max(array)\n",
    "    return (array - amin) / (amax - amin)\n",
    "def postprocess(res):\n",
    "    res_new = res\n",
    "    res = measure.label(res, connectivity=2)\n",
    "    num = res.max()\n",
    "    for i in range(1, num+1):\n",
    "        idy, idx = np.where(res==i)\n",
    "        if len(idy) <= 20:\n",
    "            res_new[idy, idx] = 0\n",
    "    return res_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input the Yellow River Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './'\n",
    "data_traingt = sio.loadmat(os.path.join('mask_train.mat'))['mask_train']\n",
    "data_testgt = sio.loadmat(os.path.join( 'mask_test.mat'))['mask_test']\n",
    "im1 = sio.loadmat(os.path.join('data_1.mat'))['data']\n",
    "im2 = sio.loadmat(os.path.join('data_2.mat'))['data']\n",
    "\n",
    "im1 = im1.reshape(im1.shape[0], im1.shape[1], 1)\n",
    "im2 = im2.reshape(im2.shape[0], im2.shape[1], 1)\n",
    "\n",
    "height , width, c = im1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input the Ottawa Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path_gt = \"ottawa_gt.bmp\"\n",
    "# im_gt = Image.open(image_path_gt)\n",
    "# im_gt = im_gt.convert('L')\n",
    "# im_gt = np.array(im_gt)\n",
    "# im_gt[im_gt == 255] = 1\n",
    "\n",
    "# data_traingt = im_gt\n",
    "# data_testgt = im_gt\n",
    "\n",
    "# image_path1 = \"ottawa_1.bmp\"\n",
    "# im1 = Image.open(image_path1)\n",
    "# im1_gray = im1.convert('L')\n",
    "# im1_array = np.array(im1_gray)\n",
    "# im1_array[im1_array == 255] = 1\n",
    "# # Reshape the NumPy array to add an extra dimension\n",
    "# im1 = im1_array.reshape(im1_array.shape[0], im1_array.shape[1], 1)\n",
    "\n",
    "# # print(im1.shape)\n",
    "\n",
    "# image_path2 = \"ottawa_2.bmp\"\n",
    "# im2 = Image.open(image_path2)\n",
    "# im2_gray = im2.convert('L')\n",
    "# im2_array = np.array(im2_gray)\n",
    "# im2_array[im2_array == 255] = 1\n",
    "# # Reshape the NumPy array to add an extra dimension\n",
    "# im2 = im2_array.reshape(im2_array.shape[0], im2_array.shape[1], 1)\n",
    "\n",
    "\n",
    "# data_traingt = np.array(im_gt)\n",
    "# data_testgt = np.array(im_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3424,
     "status": "ok",
     "timestamp": 1711468257914,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "fjjz-kyvbxDT",
    "outputId": "381b3af1-c0b6-4152-b1b9-f2b9dccfbad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniques  [0 1 2]\n",
      "uniques  [0 1 2]\n",
      "uniques  [1 2]\n",
      "uniques  [1 2]\n",
      "torch.Size([20220, 1, 7, 7])\n",
      "torch.Size([5055, 1, 7, 7])\n",
      "torch.Size([89046, 1, 7, 7])\n",
      "Creating dataloader\n",
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "windowSize = 7 # patch size\n",
    "class_num = 2\n",
    "testRatio = 0.2 # the ratio of Validation set\n",
    "trainRatio = 0.9 # the ratio of Training set selected from preclassification\n",
    "\n",
    "\n",
    "# All pseudo-label set\n",
    "train_1, labels ,_ = createImgCube(im1, data_traingt, createPosWithoutZero(im1, data_traingt), windowSize=windowSize)\n",
    "train_2, _ ,_ = createImgCube(im2, data_traingt, createPosWithoutZero(im2, data_traingt), windowSize=windowSize)\n",
    "\n",
    "# training set selected from pseudo-label set\n",
    "train_1, _, train_labels, _ = splitTrainTestSet(train_1, labels, trainRatio, randomState=111)\n",
    "train_2, _, _, _ = splitTrainTestSet(train_2, labels, trainRatio, randomState=111)\n",
    "\n",
    "# data augmentation if need\n",
    "Xh = []\n",
    "Xl = []\n",
    "y = []\n",
    "for i in range(train_1.shape[0]):\n",
    "    Xh.append(train_1[i])\n",
    "    Xl.append(train_2[i])\n",
    "\n",
    "    noise = np.random.normal(0.0, 0.01, size=train_1[0].shape)\n",
    "    noise2 = np.random.normal(0.0, 0.01, size=train_2[0].shape)\n",
    "    Xh.append(np.flip(train_1[i] + noise, axis=1))\n",
    "    Xl.append(np.flip(train_2[i] + noise2, axis=1))\n",
    "\n",
    "    k = np.random.randint(4)\n",
    "    Xh.append(np.rot90(train_1[i], k=k))\n",
    "    Xl.append(np.rot90(train_2[i], k=k))\n",
    "\n",
    "    y.append(train_labels[i])\n",
    "    y.append(train_labels[i])\n",
    "    y.append(train_labels[i])\n",
    "\n",
    "labels = np.asarray(y, dtype=np.int8)\n",
    "train_1 = np.asarray(Xh, dtype=np.float32)\n",
    "train_2 = np.asarray(Xl,dtype=np.float32)\n",
    "train_1 = torch.from_numpy(train_1.transpose(0,3,1,2)).float()\n",
    "train_2 = torch.from_numpy(train_2.transpose(0,3,1,2)).float()\n",
    "\n",
    "# Select a partial validation set from the training set\n",
    "X_train, X_val, train_labels, val_labels = splitTrainTestSet(train_1, labels, testRatio, randomState=111)\n",
    "X_train_2, X_val_2, _, _ = splitTrainTestSet(train_2, labels, testRatio, randomState=111)\n",
    "\n",
    "# testing set\n",
    "X_test, test_labels ,_ = createImgCube(im1, data_traingt, createPosWithoutZero(im1, data_testgt), windowSize=windowSize)\n",
    "X_test_2, _ ,_ = createImgCube(im2, data_traingt, createPosWithoutZero(im2, data_testgt), windowSize=windowSize)\n",
    "X_test = torch.from_numpy(X_test.transpose(0,3,1,2)).float()\n",
    "X_test_2 = torch.from_numpy(X_test_2.transpose(0,3,1,2)).float()\n",
    "\n",
    "print (X_train.shape)\n",
    "print (X_val.shape)\n",
    "print (X_test.shape)\n",
    "print(\"Creating dataloader\")\n",
    "print(np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Ottawa Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" Training dataset\"\"\"\n",
    "# class TrainDS(torch.utils.data.Dataset):\n",
    "#     def __init__(self):\n",
    "#         self.len = train_labels.shape[0]\n",
    "#         self.hsi = torch.FloatTensor(X_train)\n",
    "#         self.lidar = torch.FloatTensor(X_train_2)\n",
    "#         self.labels = torch.LongTensor(train_labels)\n",
    "#     def __getitem__(self, index):\n",
    "#         return self.hsi[index], self.lidar[index], self.labels[index]\n",
    "#     def __len__(self):\n",
    "#         return self.len\n",
    "\n",
    "# \"\"\" Testing dataset\"\"\"\n",
    "# class TestDS(torch.utils.data.Dataset):\n",
    "#     def __init__(self):\n",
    "#         self.len = test_labels.shape[0]\n",
    "#         self.hsi = torch.FloatTensor(X_test)\n",
    "#         self.lidar = torch.FloatTensor(X_test_2)\n",
    "#         self.labels = torch.LongTensor(test_labels)\n",
    "#     def __getitem__(self, index):\n",
    "#         return self.hsi[index], self.lidar[index], self.labels[index]\n",
    "#     def __len__(self):\n",
    "#         return self.len\n",
    "# \"\"\" Validation dataset\"\"\"\n",
    "# class ValDS(torch.utils.data.Dataset):\n",
    "#     def __init__(self):\n",
    "#         self.len = val_labels.shape[0]\n",
    "#         self.hsi = torch.FloatTensor(X_val)\n",
    "#         self.lidar = torch.FloatTensor(X_val_2)\n",
    "#         self.labels = torch.LongTensor(val_labels )\n",
    "#     def __getitem__(self, index):\n",
    "#         return self.hsi[index], self.lidar[index], self.labels[index]\n",
    "#     def __len__(self):\n",
    "#         return self.len\n",
    "\n",
    "\n",
    "# # generate trainloader and valloader\n",
    "# trainset = TrainDS()\n",
    "# testset  = TestDS()\n",
    "# valset = ValDS()\n",
    "# train_loader = torch.utils.data.DataLoader(dataset = trainset, batch_size = 64, shuffle = True, num_workers = 0, drop_last = True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset = testset, batch_size = 64, shuffle = False, num_workers = 0, drop_last = True)\n",
    "# val_loader = torch.utils.data.DataLoader(dataset = valset, batch_size = 64, shuffle = False, num_workers = 0, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Yellow River Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Training dataset\"\"\"\n",
    "class TrainDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = train_labels.shape[0]\n",
    "        self.hsi = torch.FloatTensor(X_train)\n",
    "        self.lidar = torch.FloatTensor(X_train_2)\n",
    "        self.labels = torch.LongTensor(train_labels - 1)\n",
    "    def __getitem__(self, index):\n",
    "        return self.hsi[index], self.lidar[index], self.labels[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\"\"\" Testing dataset\"\"\"\n",
    "class TestDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = test_labels.shape[0]\n",
    "        self.hsi = torch.FloatTensor(X_test)\n",
    "        self.lidar = torch.FloatTensor(X_test_2)\n",
    "        self.labels = torch.LongTensor(test_labels - 1)\n",
    "    def __getitem__(self, index):\n",
    "        return self.hsi[index], self.lidar[index], self.labels[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\"\"\" Validation dataset\"\"\"\n",
    "class ValDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = val_labels.shape[0]\n",
    "        self.hsi = torch.FloatTensor(X_val)\n",
    "        self.lidar = torch.FloatTensor(X_val_2)\n",
    "        self.labels = torch.LongTensor(val_labels - 1)\n",
    "    def __getitem__(self, index):\n",
    "        return self.hsi[index], self.lidar[index], self.labels[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate trainloader and valloader\n",
    "trainset = TrainDS()\n",
    "testset  = TestDS()\n",
    "valset = ValDS()\n",
    "train_loader = torch.utils.data.DataLoader(dataset = trainset, batch_size = 64, shuffle = True, num_workers = 0, drop_last = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = testset, batch_size = 64, shuffle = False, num_workers = 0, drop_last = True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = valset, batch_size = 64, shuffle = False, num_workers = 0, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniques  [1 2]\n"
     ]
    }
   ],
   "source": [
    "changed_pixels_positions = createPosWithoutZero(im1, data_testgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQAMHkFinTkl"
   },
   "source": [
    "# 6. Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1711468264630,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "mnxnPMpxb0XA"
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((label) * torch.pow(euclidean_distance, 2)+(1-label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n",
    "\n",
    "def calc_loss(x1, x2, outputs, labels, alpha):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss1 = criterion(outputs, labels)\n",
    "\n",
    "    contrastive = ContrastiveLoss()\n",
    "    loss2 = contrastive(x1, x2, labels)\n",
    "\n",
    "    loss_sum = loss1 + alpha* loss2\n",
    "    return loss_sum\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (inputs_1, inputs_2, labels) in enumerate(train_loader):\n",
    "\n",
    "        inputs_1, inputs_2 = inputs_1.to(device), inputs_2.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        feat_1, feat_2, outputs = model(inputs_1, inputs_2)\n",
    "        loss = calc_loss(feat_1, feat_2, outputs, labels, alpha = 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print('[Epoch: %d]   [loss avg: %.4f]   [current loss: %.4f]' %(epoch + 1, total_loss/(epoch+1), loss.item()))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    predicted_changes = []\n",
    "    test_labels_list = []\n",
    "    for inputs_1, inputs_2, labels in test_loader:\n",
    "        inputs_1, inputs_2 = inputs_1.to(device), inputs_2.to(device)\n",
    "        _, _, outputs = model(inputs_1, inputs_2)\n",
    "        \n",
    "        outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "        predicted_changes.extend(outputs)\n",
    "        test_labels_list.extend(labels.numpy())\n",
    "\n",
    "    predicted_changes = np.array(predicted_changes)\n",
    "    test_labels = np.array(test_labels_list)\n",
    "\n",
    "    # Calculate metrics\n",
    "    FP = np.sum((predicted_changes == 1) & (test_labels == 0))\n",
    "    FN = np.sum((predicted_changes == 0) & (test_labels == 1))\n",
    "    OE = (FP + FN) / len(test_labels) * 100\n",
    "    PCC = 100 - OE\n",
    "    TP = np.sum((predicted_changes == 1) & (test_labels == 1))\n",
    "    TN = np.sum((predicted_changes == 0) & (test_labels == 0))\n",
    "\n",
    "    total = len(test_labels)\n",
    "    positive_actual = np.sum(test_labels == 1)\n",
    "    positive_predicted = np.sum(predicted_changes == 1)\n",
    "    negative_actual = total - positive_actual\n",
    "    negative_predicted = total - positive_predicted\n",
    "    Pe = ((positive_actual / total) * (positive_predicted / total)) + ((negative_actual / total) * (negative_predicted / total))\n",
    "    P0 = (TP + TN) / total\n",
    "    P_e = (positive_actual / total) ** 2 + (negative_actual / total) ** 2\n",
    "    kappa = (P0 - Pe) / (1 - Pe)\n",
    "\n",
    "    # Class-wise precision, recall, and F1 score\n",
    "    precision_0 = precision_score(test_labels, predicted_changes, labels=[0], average=None)[0]\n",
    "    recall_0 = recall_score(test_labels, predicted_changes, labels=[0], average=None)[0]\n",
    "    f1_0 = f1_score(test_labels, predicted_changes, labels=[0], average=None)[0]\n",
    "\n",
    "    precision_1 = precision_score(test_labels, predicted_changes, labels=[1], average=None)[0]\n",
    "    recall_1 = recall_score(test_labels, predicted_changes, labels=[1], average=None)[0]\n",
    "    f1_1 = f1_score(test_labels, predicted_changes, labels=[1], average=None)[0]\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = (TP + TN) / len(test_labels) * 100\n",
    "\n",
    "    return FP, FN, OE, PCC, TP, TN, precision_0, recall_0, f1_0, precision_1, recall_1, f1_1, accuracy, predicted_changes ,kappa\n",
    "\n",
    "def test2(model, device, test_loader):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    for inputs_1, inputs_2, labels in test_loader:\n",
    "\n",
    "        inputs_1, inputs_2 = inputs_1.to(device), inputs_2.to(device)\n",
    "        _, _, outputs = model(inputs_1, inputs_2)\n",
    "        outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "\n",
    "        if count == 0:\n",
    "            y_pred_test =  outputs\n",
    "            test_labels = labels\n",
    "            count = 1\n",
    "        else:\n",
    "            y_pred_test = np.concatenate( (y_pred_test, outputs) )\n",
    "            test_labels = np.concatenate( (test_labels, labels) )\n",
    "    a = 0\n",
    "    for c in range(len(y_pred_test)):\n",
    "        if test_labels[c]==y_pred_test[c]:\n",
    "            a = a+1\n",
    "    acc = a/len(y_pred_test)*100\n",
    "    print('%.2f' %(a/len(y_pred_test)*100))\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available, else use CPU\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "image_height = image_width = 64\n",
    "image_channel = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Net().to(device)\n",
    "# model = SCNNModel(image_height, image_width, image_channel)\n",
    "model = SCNNAttentionModel(image_height, image_width, image_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1]   [loss avg: 4296.1196]   [current loss: 0.7296]\n",
      "97.68\n",
      "Save model!\n",
      "[Epoch: 2]   [loss avg: 13.5654]   [current loss: 0.0088]\n",
      "99.64\n",
      "Save model!\n",
      "[Epoch: 3]   [loss avg: 3.9636]   [current loss: 0.0052]\n",
      "99.56\n",
      "[Epoch: 4]   [loss avg: 3.2497]   [current loss: 0.4948]\n",
      "98.42\n",
      "[Epoch: 5]   [loss avg: 2.1419]   [current loss: 0.0128]\n",
      "98.86\n",
      "[Epoch: 6]   [loss avg: 1.1131]   [current loss: 0.0051]\n",
      "99.84\n",
      "Save model!\n",
      "[Epoch: 7]   [loss avg: 0.9199]   [current loss: 0.0062]\n",
      "99.88\n",
      "Save model!\n",
      "[Epoch: 8]   [loss avg: 0.5116]   [current loss: 0.0098]\n",
      "99.50\n",
      "[Epoch: 9]   [loss avg: 0.4488]   [current loss: 0.0117]\n",
      "99.90\n",
      "Save model!\n",
      "[Epoch: 10]   [loss avg: 0.9165]   [current loss: 0.0026]\n",
      "99.84\n",
      "[Epoch: 11]   [loss avg: 0.4055]   [current loss: 0.0020]\n",
      "99.86\n",
      "[Epoch: 12]   [loss avg: 1.5031]   [current loss: 0.0938]\n",
      "99.72\n",
      "[Epoch: 13]   [loss avg: 3.3427]   [current loss: 0.0440]\n",
      "99.84\n",
      "[Epoch: 14]   [loss avg: 0.6262]   [current loss: 0.0023]\n",
      "99.90\n",
      "Save model!\n",
      "[Epoch: 15]   [loss avg: 0.4347]   [current loss: 0.0126]\n",
      "99.84\n",
      "[Epoch: 16]   [loss avg: 0.3158]   [current loss: 0.0022]\n",
      "99.88\n",
      "[Epoch: 17]   [loss avg: 0.2597]   [current loss: 0.0126]\n",
      "99.82\n",
      "[Epoch: 18]   [loss avg: 0.2666]   [current loss: 0.0023]\n",
      "99.62\n",
      "[Epoch: 19]   [loss avg: 0.2995]   [current loss: 0.0027]\n",
      "99.78\n",
      "[Epoch: 20]   [loss avg: 0.2481]   [current loss: 0.0017]\n",
      "99.88\n",
      "[Epoch: 21]   [loss avg: 0.0932]   [current loss: 0.0033]\n",
      "99.84\n",
      "[Epoch: 22]   [loss avg: 732577414.3927]   [current loss: 469229.8438]\n",
      "96.13\n",
      "[Epoch: 23]   [loss avg: 1398836.2898]   [current loss: 7502.6001]\n",
      "98.04\n",
      "[Epoch: 24]   [loss avg: 325835.1083]   [current loss: 7919.4248]\n",
      "97.66\n",
      "[Epoch: 25]   [loss avg: 176261.4009]   [current loss: 22953.3086]\n",
      "98.26\n",
      "[Epoch: 26]   [loss avg: 113666.3375]   [current loss: 8505.0400]\n",
      "98.36\n",
      "[Epoch: 27]   [loss avg: 76642.4829]   [current loss: 4907.1123]\n",
      "98.62\n",
      "[Epoch: 28]   [loss avg: 56106.3820]   [current loss: 9083.4395]\n",
      "98.46\n",
      "[Epoch: 29]   [loss avg: 41942.9181]   [current loss: 10333.3799]\n",
      "98.38\n",
      "[Epoch: 30]   [loss avg: 31744.4102]   [current loss: 1416.7876]\n",
      "98.46\n",
      "[Epoch: 31]   [loss avg: 24867.0976]   [current loss: 128.4809]\n",
      "98.36\n",
      "[Epoch: 32]   [loss avg: 19478.0598]   [current loss: 0.0000]\n",
      "98.64\n",
      "[Epoch: 33]   [loss avg: 15631.3507]   [current loss: 0.0004]\n",
      "98.46\n",
      "[Epoch: 34]   [loss avg: 12570.3545]   [current loss: 351.1028]\n",
      "98.90\n",
      "[Epoch: 35]   [loss avg: 10398.0562]   [current loss: 85.9847]\n",
      "98.84\n",
      "[Epoch: 36]   [loss avg: 8507.2989]   [current loss: 960.7908]\n",
      "98.86\n",
      "[Epoch: 37]   [loss avg: 6827.5519]   [current loss: 274.4946]\n",
      "98.86\n",
      "[Epoch: 38]   [loss avg: 5911.7842]   [current loss: 1154.5299]\n",
      "99.00\n",
      "[Epoch: 39]   [loss avg: 4917.8362]   [current loss: 419.5155]\n",
      "98.94\n",
      "[Epoch: 40]   [loss avg: 4065.0795]   [current loss: 441.9766]\n",
      "98.88\n",
      "[Epoch: 41]   [loss avg: 3441.8626]   [current loss: 1667.6467]\n",
      "98.44\n",
      "[Epoch: 42]   [loss avg: 2809.4063]   [current loss: 130.0073]\n",
      "99.00\n",
      "[Epoch: 43]   [loss avg: 2341.6332]   [current loss: 40.7709]\n",
      "98.76\n",
      "[Epoch: 44]   [loss avg: 2004.5266]   [current loss: 579.7912]\n",
      "98.92\n",
      "[Epoch: 45]   [loss avg: 1666.5947]   [current loss: 189.6291]\n",
      "99.00\n",
      "[Epoch: 46]   [loss avg: 1470.0150]   [current loss: 523.8907]\n",
      "99.00\n",
      "[Epoch: 47]   [loss avg: 1231.8512]   [current loss: 15.0141]\n",
      "98.90\n",
      "[Epoch: 48]   [loss avg: 1044.4695]   [current loss: 16.1750]\n",
      "98.72\n",
      "[Epoch: 49]   [loss avg: 892.4225]   [current loss: 138.8400]\n",
      "97.90\n",
      "[Epoch: 50]   [loss avg: 803.0252]   [current loss: 192.4467]\n",
      "98.74\n"
     ]
    }
   ],
   "source": [
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "model.eval().to(device)\n",
    "params_to_update = list(model.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=lr, betas=betas)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum,weight_decay=0.0005)\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    # validation accuracy\n",
    "    acc = test2(model, device, val_loader)\n",
    "    if acc >= best_acc:\n",
    "        best_acc = acc\n",
    "        print(\"Save model!\")\n",
    "        torch.save(model.state_dict(),'model/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbpq604ro7hW"
   },
   "source": [
    "# 7. Record the final change map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.39252336448598\n",
      "False Positives: 30\n",
      "False Negatives: 172\n",
      "Overall Error (%): 0.22690510424155283\n",
      "Positive Correct Classification (%): 99.77309489575845\n",
      "Class 0 Precision: 0.9519080286645565\n",
      "Class 0 Recall: 0.9996292909571708\n",
      "Class 0 F1 Score: 0.9751851918822969\n",
      "Class 1 Precision: 0.7760455332838406\n",
      "Class 1 Recall: 0.9480048367593712\n",
      "Class 1 F1 Score: 0.8534494489046129\n",
      "Kappa: 0.2917827783593786\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model/model.pth'))\n",
    "FP, FN, OE, PCC, TP, TN, precision_0, recall_0, f1_0, precision_1, recall_1, f1_1, accuracy, predicted_changes, kappa = test(model, device, test_loader)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"False Positives: {FP}\")\n",
    "print(f\"False Negatives: {FN}\")\n",
    "print(f\"Overall Error (%): {OE}\")\n",
    "print(f\"Positive Correct Classification (%): {PCC}\")\n",
    "print(f\"Class 0 Precision: {precision_0}\")\n",
    "print(f\"Class 0 Recall: {recall_0}\")\n",
    "print(f\"Class 0 F1 Score: {f1_0}\")\n",
    "print(f\"Class 1 Precision: {precision_1}\")\n",
    "print(f\"Class 1 Recall: {recall_1}\")\n",
    "print(f\"Class 1 F1 Score: {f1_1}\")\n",
    "print(f\"Kappa: {kappa}\")\n",
    "\n",
    "binary_image = np.full_like(im1, 0, dtype=np.uint8)\n",
    "\n",
    "# Map predicted changes to the corresponding indices in changed_pixels_positions\n",
    "for idx, value in enumerate(predicted_changes):\n",
    "    # print(idx, value)\n",
    "    if value == 0:\n",
    "        y, x = changed_pixels_positions[idx]\n",
    "        binary_image[y, x] = 0  # Set as black for unchanged pixels\n",
    "    else:\n",
    "        y, x = changed_pixels_positions[idx]\n",
    "        binary_image[y, x] = 255  # Set as white for changed pixels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualize the binary image\n",
    "plt.imshow(binary_image[:, :, 0], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Predicted Change Map')\n",
    "# plt.savefig(\"SCNN_Attention_YELLOW_RIVER.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
