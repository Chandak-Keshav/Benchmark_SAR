{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1711468190725,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "D5_qsP5fCBKm"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install scipy\n",
    "# !pip install random\n",
    "# !pip install pandas\n",
    "# !pip install math\n",
    "# !pip install os\n",
    "# !pip install scikit-learn\n",
    "# !pip install torch\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1711468191109,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "RkUv6C2AaqOT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch, math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import reduce\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from skimage import io, measure\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, concatenate, Reshape, LeakyReLU\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_ySW9czhD5Y"
   },
   "source": [
    "Setting the seed of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1711468191109,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "5cuA86GuhESf"
   },
   "outputs": [],
   "source": [
    "# def seed_torch(seed = 123):\n",
    "#     random.seed(seed)\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# seed_torch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qIEQpWvnJj2"
   },
   "source": [
    "# 2. SAFNet Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1711468191109,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "SVRw577qaqzU"
   },
   "outputs": [],
   "source": [
    "class FeatNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1, 1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv1_1 = nn.Conv2d(16, 16, 3, 1, 1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(16)\n",
    "        self.conv1_2 = nn.Conv2d(16, 16, 3, 1, 1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, 1 ,2, 1)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(32, 32, 3, 1, 1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(32)\n",
    "        self.conv2_2 = nn.Conv2d(32, 32, 3, 1, 1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, 1, 2, 1)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(64)\n",
    "        self.conv3_2 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Feature fusion\n",
    "        self.conv_fusion1 = nn.Conv2d(16, 64, 1, 4, 2)\n",
    "        self.conv_fusion2 = nn.Conv2d(32, 64, 1, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x = F.relu(self.bn1_1(self.conv1_1(x1)))\n",
    "        x_1 = F.relu(self.bn1_2(self.conv1_2(x)))\n",
    "        x = x1 + x_1\n",
    "        x2 = self.conv2(x)\n",
    "        x = F.relu(self.bn2_1(self.conv2_1(x2)))\n",
    "        x_2 = F.relu(self.bn2_2(self.conv2_2(x)))\n",
    "        x = x2 + x_2\n",
    "        x3 = self.conv3(x)\n",
    "        x = F.relu(self.bn3_1(self.conv3_1(x3)))\n",
    "        x_3 = F.relu(self.bn3_2(self.conv3_2(x)))\n",
    "        return x_1, x_2, x_3\n",
    "\n",
    "class FeatFuse(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatFuse, self).__init__()\n",
    "\n",
    "        self.conv_fusion1 = nn.Conv2d(16, 64, 1, 4, 3)\n",
    "        self.conv_fusion2 = nn.Conv2d(32, 64, 1, 2, 1)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(64, 8)\n",
    "        self.fc2 = nn.Linear(8, 64*3)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "\n",
    "        batch_size = x1.size(0)\n",
    "        out_channels = x3.size(1)\n",
    "        x1 = self.conv_fusion1(x1)\n",
    "        x2 = self.conv_fusion2(x2)\n",
    "        output = []\n",
    "        output.append(x1)\n",
    "        output.append(x2)\n",
    "        output.append(x3)\n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        a_b = x.reshape(batch_size, 3, out_channels, -1)\n",
    "        a_b = self.softmax(a_b)\n",
    "        #the part of selection\n",
    "        a_b = list(a_b.chunk(3, dim=1))#split to a and b\n",
    "        a_b = list(map(lambda x:x.reshape(batch_size, out_channels, 1, 1), a_b))\n",
    "        V = list(map(lambda x,y:x*y, output, a_b))\n",
    "        V = reduce(lambda x,y:x+y, V)\n",
    "        return V\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.featnet = FeatNet()\n",
    "        self.featfuse = FeatFuse()\n",
    "        self.featnet1 = FeatNet()\n",
    "        self.featfuse1 = FeatFuse()\n",
    "        #self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(64, 2)\n",
    "\n",
    "        self.global_pool1 = nn.AdaptiveAvgPool2d(1)\n",
    "        self.global_pool2 = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(64, 2)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        x1_1, x1_2, x1_3 = self.featnet(x)\n",
    "        x2_1, x2_2, x2_3 = self.featnet1(y)\n",
    "\n",
    "        feat_11 = self.featfuse(x1_1, x1_2, x1_3)\n",
    "        feat_22 = self.featfuse1(x2_1, x2_2, x2_3)\n",
    "        feat_1 = self.global_pool1(feat_11)\n",
    "        feat_2 = self.global_pool2(feat_22)\n",
    "        feat_1 = feat_1.view(feat_1.size(0), -1)\n",
    "        feat_2 = feat_2.view(feat_2.size(0), -1)\n",
    "        feat_1 = self.fc1(feat_1)\n",
    "        feat_2 = self.fc2(feat_2)\n",
    "\n",
    "        feature_corr = self.xcorr_depthwise(feat_11, feat_22)\n",
    "        feat = feature_corr.view(feature_corr.size(0), -1)\n",
    "        #feat = global_pool(feature_corr)\n",
    "        feat = self.fc(feat)\n",
    "        return feat_1, feat_2, feat\n",
    "\n",
    "    def xcorr_depthwise(self, x, kernel):\n",
    "\n",
    "        batch = kernel.size(0)\n",
    "        channel = kernel.size(1)\n",
    "        x = x.view(1, batch*channel, x.size(2), x.size(3))\n",
    "        kernel = kernel.view(batch*channel, 1, kernel.size(2), kernel.size(3))\n",
    "        out = F.conv2d(x, kernel, groups=batch*channel)\n",
    "        out = out.view(batch, channel, out.size(2), out.size(3))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1711468191110,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "iLsdHg0ZqjhO"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCNNModel(nn.Module):\n",
    "    def __init__(self, image_height, image_width, image_channel):\n",
    "        super(SCNNModel, self).__init__()\n",
    "\n",
    "        self.activation_fn = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Convolutional block\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # Separate paths for SAR image 1\n",
    "        self.sar_output1 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),  \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),   \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),    \n",
    "            self.activation_fn,\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Separate paths for SAR image 2\n",
    "        self.sar_output2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),  \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),   \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),    \n",
    "            self.activation_fn,\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Pairwise skipped connection\n",
    "        self.sar_output1_skip = nn.Linear(128, 256)\n",
    "        self.sar_output2_skip = nn.Linear(128, 256)\n",
    "\n",
    "        # Fully connected layers for prediction\n",
    "        self.fc_prediction = nn.Sequential(\n",
    "            nn.Linear(32 * 4 * 2, 512),  # Adjusted input size\n",
    "            self.activation_fn,\n",
    "            nn.Linear(512, 256),           # Adjusted output channels\n",
    "            self.activation_fn,\n",
    "            nn.Linear(256, 2)              # Output size is (batch_size, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, sar_input1, sar_input2):\n",
    "        # Convolutional block\n",
    "        sar_output1 = self.conv_block(sar_input1)\n",
    "        sar_output2 = self.conv_block(sar_input2)\n",
    "\n",
    "        # Separate paths for SAR image 1\n",
    "        sar_output1 = self.sar_output1(sar_output1)\n",
    "\n",
    "        # Separate paths for SAR image 2\n",
    "        sar_output2 = self.sar_output2(sar_output2)\n",
    "\n",
    "        # Pairwise skipped connection\n",
    "        sar_output1_skip = self.sar_output1_skip(sar_output1)\n",
    "        merged_output = torch.cat((sar_output2, sar_output1_skip), dim=1)\n",
    "\n",
    "        # Pairwise skipped connection\n",
    "        sar_output2_skip = self.sar_output2_skip(sar_output2)\n",
    "        merged_output = torch.cat((merged_output, sar_output2_skip), dim=1)\n",
    "\n",
    "        # Fully connected layers for prediction\n",
    "        merged_output = torch.cat((sar_output1, sar_output2), dim=1)\n",
    "        predictions = self.fc_prediction(merged_output)\n",
    "\n",
    "        return sar_output1, sar_output2, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCNN with Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCNNAttentionModel(nn.Module):\n",
    "    def __init__(self, image_height, image_width, image_channel):\n",
    "        super(SCNNAttentionModel, self).__init__()\n",
    "\n",
    "        # Spatial Attention Mechanism\n",
    "        self.attention_weights = nn.Conv2d(512, 1, kernel_size=1)  # Adjust channels to match the concatenated feature maps\n",
    "        \n",
    "\n",
    "        self.activation_fn = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Convolutional block\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # Separate paths for SAR image 1\n",
    "        self.sar_output1 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),  \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),   \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),    \n",
    "            self.activation_fn,\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Separate paths for SAR image 2\n",
    "        self.sar_output2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),  \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),   \n",
    "            self.activation_fn,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),    \n",
    "            self.activation_fn,\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Pairwise skipped connection\n",
    "        self.sar_output1_skip = nn.Linear(128, 256)\n",
    "        self.sar_output2_skip = nn.Linear(128, 256)\n",
    "\n",
    "        # Fully connected layers for prediction\n",
    "        self.fc_prediction = nn.Sequential(\n",
    "            nn.Linear(32 * 4 * 2, 512),  # Adjusted input size\n",
    "            self.activation_fn,\n",
    "            nn.Linear(512, 256),           # Adjusted output channels\n",
    "            self.activation_fn,\n",
    "            nn.Linear(256, 2)              # Output size is (batch_size, 2)\n",
    "        )\n",
    "\n",
    "        self.output_reshape = lambda x: x.view(-1, image_height, image_width)\n",
    "\n",
    "    def forward(self, sar_input1, sar_input2):\n",
    "        # Convolutional block\n",
    "        sar_output1 = self.conv_block(sar_input1)\n",
    "        sar_output2 = self.conv_block(sar_input2)\n",
    "\n",
    "        # Spatial Attention Mechanism\n",
    "        attention_weights = self.attention_weights(torch.cat((sar_output1, sar_output2), dim=1))\n",
    "        sar_output1_attended = sar_output1 * attention_weights\n",
    "        sar_output2_attended = sar_output2 * attention_weights\n",
    "\n",
    "        # Separate paths for SAR image 1\n",
    "        sar_output1 = self.sar_output1(sar_output1_attended)\n",
    "\n",
    "        # Separate paths for SAR image 2\n",
    "        sar_output2 = self.sar_output2(sar_output2_attended)\n",
    "        \n",
    "        # Pairwise skipped connection\n",
    "        sar_output1_skip = self.sar_output1_skip(sar_output1)\n",
    "        merged_output = torch.cat((sar_output2, sar_output1_skip), dim=1)\n",
    "\n",
    "        # Pairwise skipped connection\n",
    "        sar_output2_skip = self.sar_output2_skip(sar_output2)\n",
    "        merged_output = torch.cat((merged_output, sar_output2_skip), dim=1)\n",
    "\n",
    "        # Fully connected layers for prediction\n",
    "        merged_output = torch.cat((sar_output1, sar_output2), dim=1)\n",
    "        predictions = self.fc_prediction(merged_output)\n",
    "\n",
    "        return sar_output1, sar_output2, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljW0mDornMLP"
   },
   "source": [
    "# 3. Data processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1711468191110,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "NOnxQkLtbtq1"
   },
   "outputs": [],
   "source": [
    "def addZeroPadding(X, margin=2):\n",
    "    newX = np.zeros((\n",
    "        X.shape[0] + 2 * margin,\n",
    "        X.shape[1] + 2 * margin,\n",
    "        X.shape[2]\n",
    "              ))\n",
    "    newX[margin:X.shape[0]+margin, margin:X.shape[1]+margin, :] = X\n",
    "    return newX\n",
    "\n",
    "def createImgCube(X ,gt ,pos:list ,windowSize=25):\n",
    "    margin = (windowSize-1)//2\n",
    "    zeroPaddingX = addZeroPadding(X, margin=margin)\n",
    "    dataPatches = np.zeros((pos.__len__(), windowSize, windowSize, X.shape[2]))\n",
    "    if( pos[-1][1]+1 != X.shape[1] ):\n",
    "        nextPos = (pos[-1][0] ,pos[-1][1]+1)\n",
    "    elif( pos[-1][0]+1 != X.shape[0] ):\n",
    "        nextPos = (pos[-1][0]+1 ,0)\n",
    "    else:\n",
    "        nextPos = (0,0)\n",
    "    return np.array([zeroPaddingX[i:i+windowSize, j:j+windowSize, :] for i,j in pos ]),\\\n",
    "    np.array([gt[i,j] for i,j in pos]) ,\\\n",
    "    nextPos\n",
    "\n",
    "def createPos(shape:tuple, pos:tuple, num:int):\n",
    "    if (pos[0]+1)*(pos[1]+1)+num >shape[0]*shape[1]:\n",
    "        num = shape[0]*shape[1]-( (pos[0])*shape[1] + pos[1] )\n",
    "    return [(pos[0]+(pos[1]+i)//shape[1] , (pos[1]+i)%shape[1] ) for i in range(num) ]\n",
    "\n",
    "def createPosWithoutZero(hsi, gt):\n",
    "    # print(\"uniques \",np.unique(gt))\n",
    "    mask = gt == 1\n",
    "    return [(i,j) for i , row  in enumerate(mask) for j , row_element in enumerate(row) if row_element]\n",
    "\n",
    "def splitTrainTestSet(X, gt, testRatio, randomState=111):\n",
    "\n",
    "    X_train, X_test, gt_train, gt_test = train_test_split(X, gt, test_size=testRatio, random_state=randomState, stratify=gt)\n",
    "    return X_train, X_test, gt_train, gt_test\n",
    "\n",
    "def createImgPatch(lidar, pos:list, windowSize=25):\n",
    "\n",
    "    margin = (windowSize-1)//2\n",
    "    zeroPaddingLidar = np.zeros((\n",
    "      lidar.shape[0] + 2 * margin,\n",
    "      lidar.shape[1] + 2 * margin\n",
    "            ))\n",
    "    zeroPaddingLidar[margin:lidar.shape[0]+margin, margin:lidar.shape[1]+margin] = lidar\n",
    "    return np.array([zeroPaddingLidar[i:i+windowSize, j:j+windowSize] for i,j in pos ])\n",
    "\n",
    "def minmax_normalize(array):\n",
    "    amin = np.min(array)\n",
    "    amax = np.max(array)\n",
    "    return (array - amin) / (amax - amin)\n",
    "def postprocess(res):\n",
    "    res_new = res\n",
    "    res = measure.label(res, connectivity=2)\n",
    "    num = res.max()\n",
    "    for i in range(1, num+1):\n",
    "        idy, idx = np.where(res==i)\n",
    "        if len(idy) <= 20:\n",
    "            res_new[idy, idx] = 0\n",
    "    return res_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input the Yellow River Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = './input_data/'\n",
    "# data_traingt = sio.loadmat(os.path.join('mask_train.mat'))['mask_train']\n",
    "# data_testgt = sio.loadmat(os.path.join( 'mask_test.mat'))['mask_test']\n",
    "# im1 = sio.loadmat(os.path.join('data_1.mat'))['data']\n",
    "# im2 = sio.loadmat(os.path.join('data_2.mat'))['data']\n",
    "\n",
    "# im1 = im1.reshape(im1.shape[0], im1.shape[1], 1)\n",
    "# im2 = im2.reshape(im2.shape[0], im2.shape[1], 1)\n",
    "\n",
    "# height , width, c = im1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input the Ottawa Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_gt = \"ottawa_gt.bmp\"\n",
    "im_gt = Image.open(image_path_gt)\n",
    "im_gt = im_gt.convert('L')\n",
    "im_gt = np.array(im_gt)\n",
    "im_gt[im_gt == 255] = 1\n",
    "\n",
    "data_traingt = im_gt\n",
    "data_testgt = im_gt\n",
    "\n",
    "image_path1 = \"ottawa_1.bmp\"\n",
    "im1 = Image.open(image_path1)\n",
    "im1_gray = im1.convert('L')\n",
    "im1_array = np.array(im1_gray)\n",
    "im1_array[im1_array == 255] = 1\n",
    "# Reshape the NumPy array to add an extra dimension\n",
    "im1 = im1_array.reshape(im1_array.shape[0], im1_array.shape[1], 1)\n",
    "\n",
    "# print(im1.shape)\n",
    "\n",
    "image_path2 = \"ottawa_2.bmp\"\n",
    "im2 = Image.open(image_path2)\n",
    "im2_gray = im2.convert('L')\n",
    "im2_array = np.array(im2_gray)\n",
    "im2_array[im2_array == 255] = 1\n",
    "# Reshape the NumPy array to add an extra dimension\n",
    "im2 = im2_array.reshape(im2_array.shape[0], im2_array.shape[1], 1)\n",
    "\n",
    "\n",
    "data_traingt = np.array(im_gt)\n",
    "data_testgt = np.array(im_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3424,
     "status": "ok",
     "timestamp": 1711468257914,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "fjjz-kyvbxDT",
    "outputId": "381b3af1-c0b6-4152-b1b9-f2b9dccfbad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3849, 1, 7, 7])\n",
      "torch.Size([963, 1, 7, 7])\n",
      "torch.Size([16049, 1, 7, 7])\n",
      "Creating dataloader\n"
     ]
    }
   ],
   "source": [
    "windowSize = 7 # patch size\n",
    "class_num = 2\n",
    "testRatio = 0.2 # the ratio of Validation set\n",
    "trainRatio = 0.9 # the ratio of Training set selected from preclassification\n",
    "\n",
    "\n",
    "# All pseudo-label set\n",
    "train_1, labels ,_ = createImgCube(im1, data_traingt, createPosWithoutZero(im1, data_traingt), windowSize=windowSize)\n",
    "train_2, _ ,_ = createImgCube(im2, data_traingt, createPosWithoutZero(im2, data_traingt), windowSize=windowSize)\n",
    "\n",
    "# training set selected from pseudo-label set\n",
    "train_1, _, train_labels, _ = splitTrainTestSet(train_1, labels, trainRatio, randomState=111)\n",
    "train_2, _, _, _ = splitTrainTestSet(train_2, labels, trainRatio, randomState=111)\n",
    "\n",
    "# data augmentation if need\n",
    "Xh = []\n",
    "Xl = []\n",
    "y = []\n",
    "for i in range(train_1.shape[0]):\n",
    "    Xh.append(train_1[i])\n",
    "    Xl.append(train_2[i])\n",
    "\n",
    "    noise = np.random.normal(0.0, 0.01, size=train_1[0].shape)\n",
    "    noise2 = np.random.normal(0.0, 0.01, size=train_2[0].shape)\n",
    "    Xh.append(np.flip(train_1[i] + noise, axis=1))\n",
    "    Xl.append(np.flip(train_2[i] + noise2, axis=1))\n",
    "\n",
    "    k = np.random.randint(4)\n",
    "    Xh.append(np.rot90(train_1[i], k=k))\n",
    "    Xl.append(np.rot90(train_2[i], k=k))\n",
    "\n",
    "    y.append(train_labels[i])\n",
    "    y.append(train_labels[i])\n",
    "    y.append(train_labels[i])\n",
    "\n",
    "labels = np.asarray(y, dtype=np.int8)\n",
    "train_1 = np.asarray(Xh, dtype=np.float32)\n",
    "train_2 = np.asarray(Xl,dtype=np.float32)\n",
    "train_1 = torch.from_numpy(train_1.transpose(0,3,1,2)).float()\n",
    "train_2 = torch.from_numpy(train_2.transpose(0,3,1,2)).float()\n",
    "\n",
    "# Select a partial validation set from the training set\n",
    "X_train, X_val, train_labels, val_labels = splitTrainTestSet(train_1, labels, testRatio, randomState=111)\n",
    "X_train_2, X_val_2, _, _ = splitTrainTestSet(train_2, labels, testRatio, randomState=111)\n",
    "\n",
    "# testing set\n",
    "X_test, test_labels ,_ = createImgCube(im1, data_traingt, createPosWithoutZero(im1, data_testgt), windowSize=windowSize)\n",
    "X_test_2, _ ,_ = createImgCube(im2, data_traingt, createPosWithoutZero(im2, data_testgt), windowSize=windowSize)\n",
    "X_test = torch.from_numpy(X_test.transpose(0,3,1,2)).float()\n",
    "X_test_2 = torch.from_numpy(X_test_2.transpose(0,3,1,2)).float()\n",
    "\n",
    "print (X_train.shape)\n",
    "print (X_val.shape)\n",
    "print (X_test.shape)\n",
    "print(\"Creating dataloader\")\n",
    "\n",
    "\"\"\" Training dataset\"\"\"\n",
    "class TrainDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = train_labels.shape[0]\n",
    "        self.hsi = torch.FloatTensor(X_train)\n",
    "        self.lidar = torch.FloatTensor(X_train_2)\n",
    "        self.labels = torch.LongTensor(train_labels - 1)\n",
    "    def __getitem__(self, index):\n",
    "        return self.hsi[index], self.lidar[index], self.labels[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\"\"\" Testing dataset\"\"\"\n",
    "class TestDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = test_labels.shape[0]\n",
    "        self.hsi = torch.FloatTensor(X_test)\n",
    "        self.lidar = torch.FloatTensor(X_test_2)\n",
    "        self.labels = torch.LongTensor(test_labels - 1)\n",
    "    def __getitem__(self, index):\n",
    "        return self.hsi[index], self.lidar[index], self.labels[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\"\"\" Validation dataset\"\"\"\n",
    "class ValDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = val_labels.shape[0]\n",
    "        self.hsi = torch.FloatTensor(X_val)\n",
    "        self.lidar = torch.FloatTensor(X_val_2)\n",
    "        self.labels = torch.LongTensor(val_labels - 1)\n",
    "    def __getitem__(self, index):\n",
    "        return self.hsi[index], self.lidar[index], self.labels[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# generate trainloader and valloader\n",
    "trainset = TrainDS()\n",
    "testset  = TestDS()\n",
    "valset = ValDS()\n",
    "train_loader = torch.utils.data.DataLoader(dataset = trainset, batch_size = 64, shuffle = True, num_workers = 0, drop_last = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = testset, batch_size = 64, shuffle = False, num_workers = 0, drop_last = True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = valset, batch_size = 64, shuffle = False, num_workers = 0, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_pixels_positions = createPosWithoutZero(im1, data_testgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQAMHkFinTkl"
   },
   "source": [
    "# 6. Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1711468264630,
     "user": {
      "displayName": "Sunny Kaushik",
      "userId": "13220963377816221870"
     },
     "user_tz": -330
    },
    "id": "mnxnPMpxb0XA"
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((label) * torch.pow(euclidean_distance, 2)+(1-label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n",
    "\n",
    "def calc_loss(x1, x2, outputs, labels, alpha):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss1 = criterion(outputs, labels)\n",
    "\n",
    "    contrastive = ContrastiveLoss()\n",
    "    loss2 = contrastive(x1, x2, labels)\n",
    "\n",
    "    loss_sum = loss1 + alpha* loss2\n",
    "    return loss_sum\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (inputs_1, inputs_2, labels) in enumerate(train_loader):\n",
    "\n",
    "        inputs_1, inputs_2 = inputs_1.to(device), inputs_2.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        feat_1, feat_2, outputs = model(inputs_1, inputs_2)\n",
    "        loss = calc_loss(feat_1, feat_2, outputs, labels, alpha = 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print('[Epoch: %d]   [loss avg: %.4f]   [current loss: %.4f]' %(epoch + 1, total_loss/(epoch+1), loss.item()))\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    predicted_changes = []\n",
    "    test_labels_list = []\n",
    "    for inputs_1, inputs_2, labels in test_loader:\n",
    "        inputs_1, inputs_2 = inputs_1.to(device), inputs_2.to(device)\n",
    "        _, _, outputs = model(inputs_1, inputs_2)\n",
    "        outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "        predicted_changes.extend(outputs)\n",
    "        test_labels_list.extend(labels.numpy())\n",
    "\n",
    "    predicted_changes = np.array(predicted_changes)\n",
    "    test_labels = np.array(test_labels_list)\n",
    "\n",
    "    # Calculate metrics\n",
    "    TP = np.sum((predicted_changes == 1) & (test_labels == 1))\n",
    "    TN = np.sum((predicted_changes == 0) & (test_labels == 0))\n",
    "    FP = np.sum((predicted_changes == 1) & (test_labels == 0))\n",
    "    FN = np.sum((predicted_changes == 0) & (test_labels == 1))\n",
    "\n",
    "    correct_predictions = (predicted_changes == test_labels)\n",
    "    accuracy = np.mean(correct_predictions) * 100\n",
    "    print('Accuracy: %.2f' % accuracy)\n",
    "\n",
    "    print(\"False Positives: \", FP)\n",
    "    print(\"False Negatives: \", FN)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(test_labels, predicted_changes, average='weighted')\n",
    "    recall = recall_score(test_labels, predicted_changes, average='weighted')\n",
    "    f1 = f1_score(test_labels, predicted_changes, average='weighted')\n",
    "\n",
    "    total = len(test_labels)\n",
    "    Pe = (np.sum(test_labels) / total) * (np.sum(predicted_changes) / total)\n",
    "    P0 = (TP + TN) / total\n",
    "\n",
    "    kappa  = -1\n",
    "\n",
    "    if Pe == 1:\n",
    "        kappa = 1  # Perfect agreement\n",
    "    elif Pe == P0:\n",
    "        kappa = 0  # Agreement by chance\n",
    "    else:\n",
    "        kappa = (P0 - Pe) / (1 - Pe)\n",
    "\n",
    "    # Print metrics\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1 Score:', f1)\n",
    "    print('Kappa: %.2f' % kappa)\n",
    "\n",
    "    return accuracy, predicted_changes\n",
    "\n",
    "\n",
    "def test2(model, device, test_loader):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    for inputs_1, inputs_2, labels in test_loader:\n",
    "\n",
    "        inputs_1, inputs_2 = inputs_1.to(device), inputs_2.to(device)\n",
    "        _, _, outputs = model(inputs_1, inputs_2)\n",
    "        outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "\n",
    "        if count == 0:\n",
    "            y_pred_test =  outputs\n",
    "            test_labels = labels\n",
    "            count = 1\n",
    "        else:\n",
    "            y_pred_test = np.concatenate( (y_pred_test, outputs) )\n",
    "            test_labels = np.concatenate( (test_labels, labels) )\n",
    "    a = 0\n",
    "    for c in range(len(y_pred_test)):\n",
    "        if test_labels[c]==y_pred_test[c]:\n",
    "            a = a+1\n",
    "    acc = a/len(y_pred_test)*100\n",
    "    print('%.2f' %(a/len(y_pred_test)*100))\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available, else use CPU\n",
    "num_epochs = 5\n",
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "image_height = image_width = 64\n",
    "image_channel = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Net().to(device)\n",
    "model = SCNNModel(image_height, image_width, image_channel)\n",
    "# model = SCNNAttentionModel(image_height, image_width, image_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1]   [loss avg: 20.9463]   [current loss: 0.0030]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 2]   [loss avg: 0.0485]   [current loss: 0.0011]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 3]   [loss avg: 0.0188]   [current loss: 0.0008]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 4]   [loss avg: 0.0100]   [current loss: 0.0006]\n",
      "100.00\n",
      "Save model!\n",
      "[Epoch: 5]   [loss avg: 0.0060]   [current loss: 0.0005]\n",
      "100.00\n",
      "Save model!\n"
     ]
    }
   ],
   "source": [
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "model.eval().to(device)\n",
    "params_to_update = list(model.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=lr, betas=betas)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum,weight_decay=0.0005)\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    # validation accuracy\n",
    "    acc = test2(model, device, val_loader)\n",
    "    if acc >= best_acc:\n",
    "        best_acc = acc\n",
    "        print(\"Save model!\")\n",
    "        torch.save(model.state_dict(),'model/model.pth')\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbpq604ro7hW"
   },
   "source": [
    "# 7. Record the final change map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "mU6Aj1eilG0D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00\n",
      "False Positives:  0\n",
      "False Negatives:  0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Kappa: 1.00\n",
      "The final accuracy is  100.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model/model.pth'))\n",
    "acc,predicted_changes = test(model, device, test_loader)\n",
    "print ('The final accuracy is ', acc)\n",
    "\n",
    "binary_image = np.full_like(im1, 255, dtype=np.uint8)\n",
    "\n",
    "# Map predicted changes to the corresponding indices in changed_pixels_positions\n",
    "for idx, value in enumerate(predicted_changes):\n",
    "    # print(idx, value)\n",
    "    if value == 0:\n",
    "        y, x = changed_pixels_positions[idx]\n",
    "        binary_image[y, x] = 0  # Set as black for unchanged pixels\n",
    "    elif value == 1:\n",
    "        y, x = changed_pixels_positions[idx]\n",
    "        binary_image[y, x] = 255  # Set as white for changed pixels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualize the binary image\n",
    "plt.imshow(binary_image[:, :, 0], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Predicted Change Map')\n",
    "# plt.savefig(\"SCNN_ON_SAFENET_OTTAWA.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
